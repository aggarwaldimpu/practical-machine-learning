---
title: "Prediction of Human Activity by Applying Machine Learning Algorithms"
author: "Jiachang (Ernest) Xu"
date: "6/23/2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(rpart)
library(randomForest)
library(e1071)
```

## Sectiong 1: Synopsis

The objective of this project is to predict human activity by applying **machine learning** algorithms.

## Section 2: Data Loading

First of all, before we do anything, we shall set the seed to 1024 for the purpose of reproducibility. Then, we shall download the training and testing datasets to the **./data** folder.

```{r download data, echo=TRUE}
## set the seed for reproducibility
set.seed(22)
## download training data
if (!file.exists("./data/training.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  destfile = "./data/training.csv")
}

## download testing data
if (!file.exists("./data/testing.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                  destfile = "./data/testing.csv")
}
```

After the training and testing datasets are downloaded, we shall read the datasets into R ready for data cleaning.

```{r read data from files, echo=TRUE}
## load the full training data reday for data slicing
data <- read.csv(file = "./data/training.csv")
## loading 20 cases of testing data for validation
validation <- read.csv(file = "./data/testing.csv")
```

## Section 3: Data Cleaning

Before actually starting the process of data cleaning, let's take a close look at **data** first.

```{r take a look at raw data, echo=TRUE}
class(data$classe)
levels(data$classe)
dim(data)
```

We can see from the output above that there exist a lot of empty spaces and NA values. Let's identify the level of NA value in **data**.

```{r identify NA level, echo=TRUE}
## identify NA level
NA.levels <- unique(apply(data, 2, function(x) {sum(is.na(x))} ))
NA.number <- dim(data)[1]-NA.levels[2]
NA.non <- NA.number/dim(data)[1]
sprintf("%1.2f%%", 100*NA.non)
```

Then, we can replace empty spaces and div0 to NA

```{r replace empty spaces, echo=TRUE}
data[data == ""] <- NA
data[data == "#DIV/0!"] <- NA
data[data == "<NA>"] <- NA
```

Now, there are no empty spaces or irregular values in **data**, we shall spitt **data** to **train** and **test**

```{r spit data, echo=TRUE}
set.seed(22)
traindex <- createDataPartition(data$classe,p = 0.8,list = FALSE)
train <- data[traindex,]
test <- data[-traindex,]
```

We split **train** to old window rows (non-aggregated).

```{r split train.data to old window rows (non-aggregated), echo=TRUE}
## select non-aggregated sensor data
train_raw <- train[which(train$new_window == "no"),]
## sensor data without NA columns (summary data)
train_raw <- train[!colSums(is.na(train)) > 0]
## test NA purity
sum(is.na(train_raw))
```

We split **train** and **test** to new window rows (aggregated), and remove NA columns and rows from the new training and testing data frames.

```{r split train.data and test.data to new window rows (aggregated)}
#Splitting data to new window rows (aggregated data)
train_sum <- train[which(train$new_window == "yes"),]
test_sum <- test[which(test$new_window == "yes"),]

#Removing full NA columns
train_sum_clean <- subset(train_sum, select=-c(kurtosis_picth_belt,kurtosis_yaw_belt,kurtosis_picth_arm,kurtosis_yaw_arm,skewness_pitch_arm,kurtosis_yaw_dumbbell,skewness_yaw_dumbbell,skewness_yaw_forearm,kurtosis_yaw_forearm,skewness_yaw_belt,skewness_roll_belt.1))

test_sum_clean <- subset(test_sum, select=-c(kurtosis_picth_belt,kurtosis_yaw_belt,kurtosis_picth_arm,kurtosis_yaw_arm,skewness_pitch_arm,kurtosis_yaw_dumbbell,skewness_yaw_dumbbell,skewness_yaw_forearm,kurtosis_yaw_forearm,skewness_yaw_belt,skewness_roll_belt.1))

#Removing NA rows
train_done <- train_sum_clean[complete.cases(train_sum_clean),]
sum(is.na(train_done))
test_done <- test_sum_clean[complete.cases(test_sum_clean),]
sum(is.na(test_done))
```

## Section 4: Machine Learning

I use random forest to build my model, because sensor data might have noise. It use bootstrap resampling for crossvalidation, and achieve a **sample error of 0.43%**

```{r model 1, echo=TRUE}
model1 <- randomForest(classe ~. , data=train_raw[,-c(1:7)], method="class")
model1
pred_test1 <- predict(model1, test)
pred_train1 <- predict(model1, train)
confusionMatrix(pred_test1, test$classe)
confusionMatrix(pred_train1, train$classe)
```

```{r model 2, eval=FALSE, include=FALSE}
features2 <- cfs(classe~.,train.old.window[,-c(1:7)])
formula2 <- as.simple.formula(features2, "classe")
fitControl <- trainControl(method = "cv", number = 3, repeats = 3)
model2 <- train(formula2, method = "rf", data =train.old.window, trControl = fitControl)
model2
pred_test2 <- predict(model2, test.data)
pred_train2 <- predict(model2, train.data)
confusionMatrix(pred_test2, test.data$classe)
confusionMatrix(pred_train2, train.data$classe)
```

```{r model 3, eval=FALSE, include=FALSE}
features3 <- cfs(classe~.,train_done[,-c(1:7)])
formula3 <- as.simple.formula(features3, "classe")
model3 <- train(formula3, method = "rf", data =train.new.clean, trControl = fitControl)
model3
pred_test3 <- predict(model3, test.new.clean)
pred_train3 <- predict(model3, train.new.clean)
confusionMatrix(pred_test3, test.new.clean$classe)
confusionMatrix(pred_train3, train.new.clean$classe)
```

## Section 5: Conclusion

```{r prediction, echo=TRUE}
predict(model1,validation)
```

